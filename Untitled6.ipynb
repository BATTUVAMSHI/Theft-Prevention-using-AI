{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3c3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a315565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52bb9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276aff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e157026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter to start1\n"
     ]
    }
   ],
   "source": [
    "source = cv2.VideoCapture(0)\n",
    "input_images = []\n",
    "targets = []\n",
    "count = 0\n",
    "for i in range(2):\n",
    "    no_of_images = 0\n",
    "    input(\"Enter to start\")\n",
    "    while 1:\n",
    "        \n",
    "        _, image = source.read(0)\n",
    "        #print(\"Captured image\")\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        detected_faces = face_detect.detectMultiScale(gray, 1.2, 9)\n",
    "        #print(\"Detected faces\",len(detected_faces))\n",
    "        if len(detected_faces) > 1:\n",
    "            continue\n",
    "        for each_face in detected_faces:\n",
    "            # Each bbox is a rectangle representing\n",
    "            # the bounding box around the detected object\n",
    "            x, y, w, h = each_face\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "            # Cropped object\n",
    "            crop = gray[y:y+h, x:x+w]\n",
    "            cropped_image = cv2.resize(crop, (100, 100))\n",
    "            input_images.append(cropped_image)\n",
    "            targets.append(count)\n",
    "            #print(\"Appends done\")\n",
    "            \n",
    "                \n",
    "            no_of_images += 1\n",
    "            cv2.putText(image, \"Image No : \" + str(no_of_images), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 100, 100), 2)\n",
    "        \n",
    "            cv2.imshow('Verzeo Project', cropped_image)\n",
    "            #cv2.imshow(\"Just Face\", crop)\n",
    "            key = cv2.waitKey(1)\n",
    "            cv2.imshow(\"Face Window\", image)\n",
    "        if no_of_images == 100:\n",
    "                break\n",
    "     \n",
    "    count += 1\n",
    "\n",
    "    cv2.destroyAllWindows()     \n",
    "source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4058c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b030986",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images_normalized = np.array(input_images)/255.0\n",
    "input_images_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bff719",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images_normalized_reshaped = input_images_normalized.reshape(1000, 100, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d475ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images_normalized_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d630d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "targets = np_utils.to_categorical(targets)\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb764e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7546fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(input_images_normalized_reshaped, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6dcdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c76f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(300, (3,3), input_shape=(100,  100, 1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(100, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = ModelCheckpoint('best_one', verbose=0, save_best_only=True)\n",
    "model.fit(X_train, Y_train, epochs = 5, callbacks=[cp], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13ffb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv2.VideoCapture(0)\n",
    "\n",
    "while 1:        \n",
    "    _, image = source.read(0)\n",
    "    #print(\"Captured image\")\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    detected_faces = face_detect.detectMultiScale(gray, 1.2, 9)\n",
    "    #print(\"Detected faces\",len(detected_faces))\n",
    "    if len(detected_faces) > 1:\n",
    "        continue\n",
    "    for each_face in detected_faces:\n",
    "        # Each bbox is a rectangle representing\n",
    "        # the bounding box around the detected object\n",
    "        x, y, w, h = each_face\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 3)\n",
    "        # Cropped object\n",
    "        crop = gray[y:y+h, x:x+w]\n",
    "        cropped_image = cv2.resize(crop, (100, 100))\n",
    "        normalized_image = cropped_image/255\n",
    "        reshaped_face = np.reshape(normalized_image, (1, 100, 100, 1))\n",
    "        result = model.predict(reshaped_face)[0]\n",
    "         if results[1] < 500:\n",
    "            confidence = int( 100 * (1 - (results[1])/400) )\n",
    "            display_string = str(confidence) + '% Confident it is User'\n",
    "            \n",
    "        cv2.putText(image, display_string, (100, 120), cv2.FONT_HERSHEY_COMPLEX, 1, (255,120,150), 2)\n",
    "        \n",
    "        if confidence > 75:\n",
    "            cv2.putText(image, \"Unlocked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "        else:\n",
    "            cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "            cv2.imshow('Face Recognition', image )\n",
    "\n",
    "    except:\n",
    "        cv2.putText(image, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.putText(image, \"Locked\", (250, 450), cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', image )\n",
    "        pass\n",
    "        \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ca1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a423a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load HAAR face classifier\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load functions\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "lst=[]\n",
    "start_time=time.time()\n",
    "elapsed_time=time.time()\n",
    "# Collect 100 samples of your face from webcam input\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if face_extractor(frame) is not None:\n",
    "        start_time = time.time()\n",
    "        face = cv2.resize(face_extractor(frame), (200, 200))\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Put count on images and display live count\n",
    "        cv2.putText(face, str(time.clock()), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('Face Cropper', face)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"Face not found\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        lst.append(elapsed_time)\n",
    "        seconds=0\n",
    "        time.sleep(1)\n",
    "        pass\n",
    "\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        \n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      \n",
    "print(\"Collecting Samples Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
